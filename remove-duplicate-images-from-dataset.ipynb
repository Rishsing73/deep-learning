{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/rishabh73/remove-duplicate-images-from-dataset?scriptVersionId=104178958\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"### Many times when collecting dataset from real world there is a chance of containg multiple copies (almost copy) of a single image with different names. It can cause an imbalance in dataset and skew the weights of model hence resulting in poor performance. \n### This notebook can be used to remove duplicate images in datasets using image hash library in a fast and efficient manner.\n\n### we will go step by step mentioning the reason for each code block","metadata":{}},{"cell_type":"code","source":"# I like to use this block to set the  display size and font of jupyter notebooks\nimport IPython.core.display as di\nfrom IPython.display import display, HTML\nfrom IPython.display import Audio\ndisplay(HTML(\"<style>.container { width:100% !important; }</style>\"))\ndi.display_html(\"\"\"\n('<style>.cell { margin-bottom: 40px !important;}</style>')\n\"\"\", raw=True)\n\nfrom IPython.display import display, HTML\ndisplay(HTML(\"<style>.container { height:100% !important; }</style>\"))\n","metadata":{"execution":{"iopub.status.busy":"2022-08-25T12:02:12.670238Z","iopub.execute_input":"2022-08-25T12:02:12.670665Z","iopub.status.idle":"2022-08-25T12:02:12.682755Z","shell.execute_reply.started":"2022-08-25T12:02:12.670632Z","shell.execute_reply":"2022-08-25T12:02:12.681369Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>.container { width:100% !important; }</style>"},"metadata":{}},{"output_type":"display_data","data":{"text/html":"\n\n('<style>.cell { margin-bottom: 40px !important;}</style>')\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>.container { height:100% !important; }</style>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Importing the libraries","metadata":{}},{"cell_type":"code","source":"# importing some usefull librariies\n\nimport glob\nfrom tqdm.auto import tqdm\nimport random\nimport pandas as pd\nimport numpy as np\nimport os\nfrom itertools import chain\nimport glob\nimport shutil\n\nfrom PIL import Image\nimport os.path\nimport imagehash\nimport multiprocessing\n\n\npd.set_option('display.max_colwidth', None)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Here we first read the dataframe containing the name of all frames, it can be a list also","metadata":{}},{"cell_type":"code","source":"df = pd.read_pickle('..../df.pkl')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's import the hash functions based upon which we will creat hash of images.\n### what is hash exactly? \n#### you must have heard of hashing in blockchain technology. consider hash as a unique small key to a very big unique lock. hashes are easy to work with because the millions of image as a data can be quite big to handle for computer but hash is just a string. where each unique hash represent a unique image so if  two hashes are same then the images (or any other data) are also same.\n\n### hence our algorithm uses a list of  hashes of given image dataset and then we can easily find duplicate hashes and remove them.\n\n### Now  the question arises how to calculate this Hash??\n\n### we will use a popular library called 'Imagehash' which has multiple algorithms available like average hash, perpetual hash etc.\n### They are baed upon the frequency property of an image. high frequencies in image represent details and low frequencies represent  structure.\n### you can read about them further here.\n\n#### https://www.hackerfactor.com/blog/index.php?/archives/432-Looks-Like-It.html\n","metadata":{}},{"cell_type":"code","source":"hashing_method = [imagehash.average_hash,imagehash.phash,imagehash.whash,imagehash.dhash] # Here defining a list of hashes we will use\n\n# lets define some useful functions\nimage_real_dir = 'path to the image folder'\n\ndef return_hash(self,row_idx): # here we assume that the column name which contains frame  name is titled 'frame_name'\n    '''\n    INPUT\n    row_idx: int; row number of dataframe containing frame name\n    \n    OUTPUT\n    hashes of image: strings\n    '''\n    Hash = [] # creating an empty list to store the hash of each image based upon all the hashing method defined above\n    img_read = Image.open(os.path.join(img_real_dir,row_idx['frame_name']))\n    for func_hash in hashing_method:\n        Hash.append(str(func_hash(img_read))) \n\n    return Hash[0],Hash[1],Hash[2],Hash[3] # returning the hash of an image based upon different algorithms\n\n# we have defined a function which will return the hashes of image and \n\n\n\n\ntqdm.pandas() # we will use multi processing to engage all cores of cpu\ndf_dum = df.copy() # keeping a copy of dataframe\nrows_iter = (row for _, row in df_dum.iterrows()) # row list \npool = multiprocessing.Pool() # initiating the multiprocessing pool\n\n# Here we map the function defined above to all the rows(frames) inside dataframe and create four columns titles hash1, hash2, hash3, hash4\n# corresponding to the hash algoithms defined above\ndf_dum['hash1'],df_dum['hash2'],df_dum['hash3'],df_dum['hash4'] = zip(*list(tqdm(pool.imap(return_hash,rows_iter),total=len(df_dum)))) \n\npool.close()\npool.join()\n\n# now we will create a set of hash1 column and remove the duplicate rows and then further apply it to remaning hashes\n\ndf_dum = df_dum.groupby(['hash1'], as_index=False)[['frame_name','hash2','hash3','hash4']].agg(lambda x: list(set(x)))\n\ndf_dum['hash2'] = df_dum['hash2'].apply(lambda x: x[0]) # this returns the remaining hashes in a list containg duplicate hashes hence we are \n# taking the first hash out of list for hash2\n\ndf_dum = df_dum.groupby(['hash2'], as_index=False)[['frame_name','hash3','hash4']].agg(lambda x: list(set(list(chain.from_iterable(x)))))\ndf_dum['hash3'] = df_dum['hash3'].apply(lambda x: x[0])\n\ndf_dum = df_dum.groupby(['hash3'], as_index=False)[['frame_name','hash4']].agg(lambda x: list(set(list(chain.from_iterable(x)))))\ndf_dum['hash4'] = df_dum['hash4'].apply(lambda x: x[0])\n\ndf_dum = df_dum.groupby(['hash4'], as_index=False)[['frame_name']].agg(lambda x: list(set(list(chain.from_iterable(x)))))\n\ndf_dum['frame_name'] = df_dum['frame_name'].apply(lambda x: x[0])\ndf_dum.drop(columns='hash4') # removing the last column\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_dum # it is the final dataframe containg unique imgaes name","metadata":{},"execution_count":null,"outputs":[]}]}